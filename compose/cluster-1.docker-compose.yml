# this compose file is for the first 8x A100 80GB cluster. It is networked with the first
# cluster using swarm networking, so containers can still refer to each other!
# see cluster-2.docker-compose.yml for more.
version: '3.8'

services:
  # TODO set up LiteLLM docker image

  # vLLM Instance 0 serving Hermes 3 8B on GPU 0
  vllm-0-hermes:
    image: vllm/vllm-openai:v0.6.3
    entrypoint: [
      "vllm", "serve", "NousResearch/Hermes-3-Llama-3.1-8B",
      "--enable-prefix-caching", "--enable-auto-tool-choice", "--enable-chunked-prefill",
      "--gpu-memory-utilization", "0.98",
      "--max-model-len", "131072",
      "--tool-call-parser", "hermes",
      "--max-num-batched-tokens", "512",
      "--max-num-seqs", "256",
      "--num-scheduler-steps", "1"
    ]
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGINGFACE_TOKEN}
    volumes:
      - /ephemeral/huggingface:/root/.cache/huggingface # storage for HF cache
      - ./configs:/usr/app # configs like chat templates, vllm configs, tool parsers
    #ports:
    #  - "8000:8000"
    networks:
      - distributed-inference-secure
    ipc: host
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: "nvidia"
              capabilities: ["gpu"]
              device_ids: ["0"] # TODO this needs to run on the other node.

  vllm-1-qwen25:
    image: vllm/vllm-openai:v0.6.3
    entrypoint: [
      "vllm", "serve", "Qwen/Qwen2.5-7B-Instruct",
      "--enable-prefix-caching", "--enable-auto-tool-choice", "--enable-chunked-prefill",
      "--gpu-memory-utilization", "0.98",
      "--max-model-len", "32768",
      "--tool-call-parser", "hermes",
      "--max-num-batched-tokens", "512",
      "--max-num-seqs", "256",
      "--num-scheduler-steps", "1"
    ]
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGINGFACE_TOKEN}
    volumes:
      - /ephemeral/huggingface:/root/.cache/huggingface # storage for HF cache
      - ./configs:/usr/app # configs like chat templates, vllm configs, tool parsers
    #ports:
    #  - "8001:8000"
    networks:
      - distributed-inference-secure
    ipc: host
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: "nvidia"
              capabilities: ["gpu"]
              device_ids: ["1"] # TODO this needs to run on the other node.
  vllm-2-llama31:
    image: vllm/vllm-openai:v0.6.3
    entrypoint: [
      "vllm", "serve", "meta-llama/Llama-3.1-8B-Instruct",
      "--enable-prefix-caching", "--enable-auto-tool-choice", "--enable-chunked-prefill",
      "--gpu-memory-utilization", "0.98",
      "--max-model-len", "131072",
      "--tool-call-parser", "llama3_json",
      "--max-num-batched-tokens", "512",
      "--max-num-seqs", "256",
      "--num-scheduler-steps", "1",
      # custom chat template with better system prompt improves performance!
      "--chat-template", "/usr/app/chat-templates/llama_3_1.jinja"
    ]
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGINGFACE_TOKEN}
    volumes:
      - /ephemeral/huggingface:/root/.cache/huggingface # storage for HF cache
      - ./configs:/usr/app # configs like chat templates, vllm configs, tool parsers
    #ports:
    #  - "8002:8000"
    networks:
      - distributed-inference-secure
    ipc: host
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: "nvidia"
              capabilities: ["gpu"]
              device_ids: ["2"] # TODO this needs to run on the other node.

  vllm-3-toolace:
    image: vllm/vllm-openai:v0.6.3
    entrypoint: [
      "vllm", "serve", "Team-ACE/ToolACE-8B",
      "--enable-prefix-caching", "--enable-auto-tool-choice", "--enable-chunked-prefill",
      "--tool-parser-plugin", "/usr/app/tool-parsers/pythonic_tool_parser.py",
      "--gpu-memory-utilization", "0.98",
      "--max-model-len", "131072",
      "--tool-call-parser", "pythonic",
      "--max-num-batched-tokens", "512",
      "--max-num-seqs", "256",
      "--num-scheduler-steps", "1",
      # custom chat template with better system prompt improves performance!
      "--chat-template", "/usr/app/chat-templates/tool_ace.jinja"
    ]
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGINGFACE_TOKEN}
    volumes:
      - /ephemeral/huggingface:/root/.cache/huggingface # storage for HF cache
      - ./configs:/usr/app # configs like chat templates, vllm configs, tool parsers
    #ports:
    #  - "8003:8000"
    networks:
      - distributed-inference-secure
    ipc: host
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: "nvidia"
              capabilities: ["gpu"]
              device_ids: ["3"] # TODO this needs to run on the other node.

  # openbmb/MiniCPM3-4B
  vllm-4-minicpm3:
    image: vllm/vllm-openai:v0.6.3
    entrypoint: [
      "vllm", "serve", "openbmb/MiniCPM3-4B",
      "--enable-prefix-caching", "--enable-auto-tool-choice", "--enable-chunked-prefill",
      "--tool-parser-plugin", "/usr/app/tool-parsers/minicpm3_tool_parser.py",
      "--gpu-memory-utilization", "0.98",
      "--max-model-len", "32768",
      "--tool-call-parser", "minicpm",
      "--max-num-batched-tokens", "512",
      "--max-num-seqs", "256",
      "--num-scheduler-steps", "1",
      # custom chat template with better system prompt improves performance!
      "--chat-template", "/usr/app/chat-templates/minicpm3_4b.jinja",
      "--trust-remote-code"
    ]
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGINGFACE_TOKEN}
    volumes:
      - /ephemeral/huggingface:/root/.cache/huggingface # storage for HF cache
      - ./configs:/usr/app # configs like chat templates, vllm configs, tool parsers
    #ports:
    #  - "8004:8000"
    networks:
      - distributed-inference-secure
    ipc: host
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: "nvidia"
              capabilities: ["gpu"]
              device_ids: ["4"] # TODO this needs to run on the other node.

  # mistral
  vllm-5-mistral:
    image: vllm/vllm-openai:v0.6.3
    entrypoint: [
      "vllm", "serve", "mistralai/Mistral-7B-Instruct-v0.3",
      # NOTE do NOT use the mistral tokenizer because it breaks tool calling
      # "--tokenizer-mode", "mistral", "--load-format", "mistral", "--config-format",  "mistral",
      "--enable-prefix-caching", "--enable-auto-tool-choice", "--enable-chunked-prefill",
      "--tool-parser", "mistral",
      "--gpu-memory-utilization", "0.98",
      "--max-model-len", "32768",
      "--tool-call-parser", "mistral",
      "--max-num-batched-tokens", "512",
      "--max-num-seqs", "256",
      "--num-scheduler-steps", "1",
      # custom chat template with better system prompt improves performance!
      "--chat-template", "/usr/app/chat-templates/mistral.jinja",
      "--trust-remote-code"
    ]
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGINGFACE_TOKEN}
    volumes:
      - /ephemeral/huggingface:/root/.cache/huggingface # storage for HF cache
      - ./configs:/usr/app # configs like chat templates, vllm configs, tool parsers
    #ports:
    #  - "8005:8000"
    networks:
      - distributed-inference-secure
    ipc: host
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: "nvidia"
              capabilities: ["gpu"]
              device_ids: ["5"] # TODO this needs to run on the other node.

  vllm-6-internlm:
    image: vllm/vllm-openai:v0.6.3
    entrypoint: [
      "vllm", "serve", "internlm/internlm2_5-7b-chat",
      "--enable-prefix-caching", "--enable-auto-tool-choice", "--enable-chunked-prefill",
      "--tool-parser", "internlm",
      "--gpu-memory-utilization", "0.98",
      "--max-model-len", "65536",
      "--tool-call-parser", "internlm",
      "--max-num-batched-tokens", "512",
      "--max-num-seqs", "256",
      "--num-scheduler-steps", "1",
      # custom chat template with better system prompt improves performance!
      "--chat-template", "/usr/app/chat-templates/internlm.jinja",
      "--trust-remote-code"
    ]
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGINGFACE_TOKEN}
    volumes:
      - /ephemeral/huggingface:/root/.cache/huggingface # storage for HF cache
      - ./configs:/usr/app # configs like chat templates, vllm configs, tool parsers
    #ports:
    #  - "8006:8000"
    networks:
      - distributed-inference-secure
    ipc: host
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: "nvidia"
              capabilities: ["gpu"]
              device_ids: ["6"] # TODO this needs to run on the other node.

  prometheus:
    image: prom/prometheus
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data-blacklight:/prometheus
      # TODO add a volume to persist prometheus data
    # ports:
      # no public port needed- "9000:9090"
    networks:
      - distributed-inference-secure
    restart: unless-stopped

  grafana:
    image: grafana/grafana
    ports:
      - "3030:3000"
    networks:
      - distributed-inference-secure
    volumes:
      - grafana-storage-blacklight:/var/lib/grafana
    restart: unless-stopped

  text-embeddings-inference:
    image: ghcr.io/huggingface/text-embeddings-inference:1.5
    deploy:
      resources:
        reservations:
          devices:
            - driver: "nvidia"
              capabilities: ["gpu"]
              device_ids: ["7"] # TODO this needs to run on the other node.
    environment:
      HF_API_TOKEN: <secret>
      MODEL_ID: jinaai/jina-embeddings-v2-base-en
    volumes:
      - /ephemeral/huggingface:/root/.cache/huggingface
    ports:
      - "8080:80"
    networks:
      - distributed-inference-secure
    restart: unless-stopped

volumes:
  grafana-storage-blacklight:
  prometheus-data-blacklight:

# attach to existing overlay network created using docker swarm
networks:
  distributed-inference-secure:
    external: true
